{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKIiY6p3GRFq"
   },
   "source": [
    "# Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7VmLUr5GTNw"
   },
   "source": [
    "A confidence interval quantifies a range of potential values that a parameter of a population (e.g., its mean value) can take, based on sampled observations (i.e., the data you have). In other words, the confidence interval is a measure of uncertainty: the wider the interval, the more uncertainty there is on the actual value of the parameter, given the data. Confidence intervals are a foundational piece of all rigorous, quantitative analyses of data and in fact have been proposed to be one of the three pillars of \"[the new statistics](https://journals.sagepub.com/doi/full/10.1177/0956797613504966)\" (the other two being measures of effect sizes and the use of meta-analyses).\n",
    "\n",
    "Exactly how confidence intervals should be interpreted is a matter of [debate](https://stats.stackexchange.com/questions/2272/whats-the-difference-between-a-confidence-interval-and-a-credible-interval) and depends (of course!) on whether it was computed using frequentist or Bayesian methods. In either case, they are computed at a pre-specified confidence level, which is typically 95% but can be other values (e.g., 90% or 99%).\n",
    "\n",
    "Using Bayesian methods, the interval (typically referred to as a \"credible interval\") is interpreted in a more intuitive manner: there is a 95% (or whatever value that is specified by the confidence level) chance that the true value is within the given interval (more specifically, 95% of the mass of the posterior probability distribution is within the given interval).\n",
    "\n",
    "In contrast, a frequentist confidence interval implies that if we repeated the same experiment N times, the true value will fall within that interval 95% of the time.\n",
    "\n",
    "The good news is that under a number of common conditions, the two approaches give very similar answers and so as a practical matter it might not matter very much which you use (e.g., [this paper](https://canvas.upenn.edu/courses/1358934/files/folder/Courses/Quantitative%20Neuro%20Core/Readings/Confidence%20Intervals%20and%20Bootstrapping/Confidence%20Intervals?preview=79166068)). The bad news is that there are conditions in which the two approaches do not give the same answers (see [here](https://pubmed.ncbi.nlm.nih.gov/26450628/) and [here](https://psyarxiv.com/we9h6)), which are worth being aware of as you compute confidence intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMdIhX4jHkIH"
   },
   "source": [
    "# Four ways to compute a confidence (or credible) interval on an estimate of the mean of a population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aur66POnHp8A"
   },
   "source": [
    "Here we assume that we have *n* measurements ($x_i$) from a population that we know is [Gaussian distributed](https://github.com/PennNGG/Quantitative-Neuroscience/blob/master/Probability%20Distributions/Python/Gaussian%20(Normal).ipynb) (i.e., with a true mean $\\mu$ and standard deviation $\\sigma$). Thus the sample mean ($\\bar{x}$) is computed as:\n",
    "\n",
    "$\\bar{x}=\\frac{\\Sigma{x_i}}{n}$.\n",
    "\n",
    "We want to find a confidence interval on our estimate of the mean. This is a very common problem, and the different approaches described below will all give roughly the same answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mqeM9UrIHOd"
   },
   "source": [
    "## 1. The simple, analytic approach with large *n* and/or known standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6OJyfDPIN1H"
   },
   "source": [
    "Here we compute the confidence interval in terms of multiples of the standard error of the mean, which is the standard deviation of the sampling distribution of the mean -- that is, the standard deviation of the distribution you would get if you repeated your experiment over and over and computed the sample mean each time. \n",
    "\n",
    "This approach requires understanding several concepts:\n",
    "\n",
    "a. Because of the [Central Limit Theorem](https://online.stat.psu.edu/stat414/book/export/html/750), the sampling distribution of the mean approaches a [normal (Gaussian)](https://github.com/PennNGG/Quantitative-Neuroscience/blob/master/Probability%20Distributions/Python/Gaussian%20(Normal).ipynb) distribution with large *n*; see [here](https://stats.stackexchange.com/questions/371067/trouble-relating-the-central-limit-theorem-to-confidence-intervals) and [here](https://online.stat.psu.edu/stat506/lesson/1/1.4) for discussions of how this idea relates to confidence intervals.\n",
    "\n",
    "b. Accordingly, we assess the variability of the sampling distribution of the mean based on its (known) standard deviation. This value is known as the standard error of the mean (SEM), which can be computed from the true standard deviation (*s*) of the sampled distribution as:\n",
    "\n",
    "$SEM=\\frac{s}{\\sqrt{n}}=\\sqrt{\\frac{\\Sigma{(x-\\bar{x})^2}}{n}}$\n",
    "\n",
    "Careful: don't [confuse *s* and the SEM](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3148365/)!\n",
    "\n",
    "c. The SEM thus indicates the range of values that are within one standard deviation of the mean value of the sampling distribution of the mean, which corresponds to ~68% of all possible values:\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1hct1RNgwYziStIetfxlUNy12bx3qpXCS)\n",
    "\n",
    "In other words, the notation $[\\bar{x}-SEM, \\bar{x}+SEM]$ indicates the lower and upper bounds of the 68% confidence interval on the mean.\n",
    "\n",
    "d. The confidence interval for an arbitrary confidence level can be computed by simply multiplying the SEM by a factor that corresponds to the appropriate range of the sampling distribution of the mean. This factor is known as the [z-score](https://www.statisticshowto.com/probability-and-statistics/z-score/) and represents the number of standard deviations that a value is from the mean of a distribution. It can be determined using a [z-score calculator](http://www.z-table.com), which relates the z-score to the area under the curve that is to the left of that z-score (i.e., the total probability that the value is less than that z-score):\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1MZVC7ez6N9w8MInWdzJl635q5L-IHf0z)\n",
    "\n",
    "For a standard, two-sided confidence interval, the appropriate z-score is determined based on one-half of one minus the confidence level, because you want to find the area between two symmetric tails.\n",
    "\n",
    "Let's say you want the 95% confidence interval (i.e., 0.95). Thus, you need to identify the z-score corresponding to an area equal to 0.5*(1â€“0.95) = 0.025, which is 1.96. Thus, the 95% confidence interval = $[\\bar{x}-SEM\\times 1.96, \\bar{x}+SEM\\times 1.96]$ \n",
    "\n",
    "You can get the z-score corresponding to a particular area under the curve to the left of that z-score in Matlab using [norminv](https://www.mathworks.com/help/stats/norminv.html) and in Python using the ppf method of Scipy's [norm](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDx_rjBGRdVX"
   },
   "source": [
    "## 2. The simple, analytic approach with small *n* and unknown population standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOAnH1rcRhAl"
   },
   "source": [
    "When the true population standard deviation is not known (as is typically the case), the SEM must be calculated using the sample standard deviation (i.e., the standard deviation estimated from the data). As detailed [here](https://colab.research.google.com/drive/1Q_Du5NK71Rc1qu-byh8dy8Fs39uvR_4n?usp=sharing), the sample standard deviation is biased unless you use [Bessel's correction](https://www.statisticshowto.com/bessels-correction/) to minimize the bias. Using this correction, the sample distribution of the mean value of a normally distributed random variable follows a [Student's t-distribution](https://colab.research.google.com/drive/1Q_Du5NK71Rc1qu-byh8dy8Fs39uvR_4n?usp=sharing) that deviates most strongly from a normal distribution when *n* is relatively small (it has heavier tails than a normal distribution, which implies that there are more extreme values). Here computing confidence intervals is the same as above, but instead of using a z-score calculator (which assumes a normal distribution), you need to use a [t-distribution calculator](https://www.dummies.com/education/math/statistics/using-the-t-distribution-to-calculate-confidence-intervals/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_S1AnwKSVje"
   },
   "source": [
    "## 3. Bootstrapped confidence intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PMjcO-sXSXT_"
   },
   "source": [
    "[Bootstrapping](https://projecteuclid.org/journals/annals-of-statistics/volume-7/issue-1/Bootstrap-Methods-Another-Look-at-the-Jackknife/10.1214/aos/1176344552.full) [available on [Canvas](https://canvas.upenn.edu/courses/1358934/files/folder/Courses/Quantitative%20Neuro%20Core/Readings/Confidence%20Intervals%20and%20Bootstrapping/Bootstrapping?preview=98867712)] is a powerful approach for estimating a sample distribution given a set of observed data. The idea is to resample (with replacement) from those data, each time computing the statistic of interest (e.g., the mean). You can then use this distribution directly to determine the confidence intervals.\n",
    "\n",
    "Advantages of this approach are that it: 1) does not require assumptions about the nature of the population distribution; 2) can be applied equally well to multiple statistics (e.g., mean or median); and 3) is easy to compute.\n",
    "\n",
    "Bootstrapping is based on the following logic (taken from this [nice description](https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading24.pdf)):\n",
    "\n",
    "1. $x_1, x_2,...,x_n$ is a data sample drawn from a distribution $F$.\n",
    "2. $\\mu$ is a statistic computed from the sample.\n",
    "3. $F^âˆ—$ is the empirical distribution of the data (the resampling distribution).\n",
    "4. $x_1^*, x_2^*,...,x_n^*$ \n",
    " is a resample of the data *of the same size as the original sample*.\n",
    "5. $\\mu^*$ is the statistic computed from the resample.\n",
    "\n",
    "The idea is that you now have a distribution $F^*$ that can be used as a surrogate for $F$; that is, confidence intervals on $\\mu$ are computed as confidence intervals on $\\mu^*$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wSAutq9_Vut6"
   },
   "source": [
    "## 4. Bayesian credible intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APQSlr4PVyG6"
   },
   "source": [
    "Credible intervals are a measure of our belief in a parameter of the population, given our data. We therefore compute the posterior probability distribution of the population mean $\\mu$ given data samples $X={x_1, x_2,...,x_n}$ according to Bayes' Rule:\n",
    "\n",
    "$p(\\mu|X)=\\frac{p(X|\\mu)p(\\mu)}{p(X)}$\n",
    "\n",
    "Here $p(X)$ represents the probability of getting the data and is really just a normalizing constant (to make sure the area under the resulting probability distribution equals one), so we'll call it *k*. $p(\\mu)$ is the prior on the mean; that is, what is the probability that the mean is equal to each particular value, determined before you see any data? We will assume this probability is uniform and thus will also refer to it as a constant, say *p*. That leaves just the likelihood term $p(X|\\mu)$, which based on our assumptions above is a Gaussian (here let us assume that we know the standard deviation $\\sigma$). Note that *X* is a set of *n* independent observations, so the total likelihood is the product of the likelihoods determined for each observation separately; that is:\n",
    "\n",
    "$p(X|\\mu)=\\prod^n_{i-1}{\\frac{1}{\\sqrt{2\\pi \\sigma^2}}exp\\left [ \\frac{-(x_i-\\mu)^2}{2\\sigma^2} \\right ]}$\n",
    "\n",
    "Which we can rearrange and multiply by $p(\\mu)=p$ and divide by $p(X)=k$ to get the posterior:\n",
    "\n",
    "$p(\\mu|X)=\\frac{p}{k\\sqrt{2\\pi \\sigma^2}}exp{\\left [\\frac{1}{2\\sigma^2} \\sum^n_{i-1}(x_i-\\mu)^2 \\right ]}$\n",
    "\n",
    "Then using the definition of the sample mean ($\\bar{x}=\\frac{1}{n}\\sum{x_i}$), [this shortcut for computing the variance](https://www.thoughtco.com/sum-of-squares-formula-shortcut-3126266), and rearranging some more gives:\n",
    "\n",
    "$p(\\mu|X)=\\frac{p}{k\\sqrt{2\\pi \\sigma^2}}exp{\\left [\\frac{1}{2\\sigma^2} \\left [ n^2\\sigma^2+n\\bar{x^2}-2n\\mu\\bar{x}^2 +n\\mu^2\\right ] \\right ]}$\n",
    "\n",
    "$p(\\mu|X)=\\frac{p}{k\\sqrt{2\\pi \\sigma^2}}exp{\\left [ -\\frac{n^2\\sigma^2}{s\\sigma^2} -\\frac{n}{2\\sigma^2}(\\mu-\\bar{x})^2\\right ]}$\n",
    "\n",
    "Note that we rearranged terms to make it clear that we are considering different values of $\\mu$ as they relate to the given sample mean $\\bar{x}$. Let's rearrange one more time:\n",
    "\n",
    "$p(\\mu|X)=\\frac{p}{k\\sqrt{2\\pi \\sigma^2}}exp{\\left [ -\\frac{n^2\\sigma^2}{s\\sigma^2}\\right ]}exp{\\left[ -\\frac{1}{2\\frac{\\sigma^2}{n}}(\\mu-\\bar{x})^2\\right ]}$\n",
    "\n",
    "This form shows that the posterior distribution for given X is itself a Gaussian with a mean value of $\\bar{x}$ and a standard deviation of $\\frac{\\sigma}{\\sqrt{n}}$; in other words, a distribution defined by the sample mean and the standard error of the mean! In this case, the credible interval is computed in exactly the same way as the confidence interval computed analytically from the sample mean and standard error of the mean described above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isP38xJSbJuA"
   },
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fys3uL-UbaYR"
   },
   "source": [
    "Compute confidence/credible intervals based on the four methods above for simulated data sampled from a population that is Gaussian distributed with mean $\\mu$=10 and standard deviation $\\sigma$=2, for *n*=5, 10, 20, 40, 80, 160, 1000 at a 95% confidence level.\n",
    "\n",
    "The answers will be found [here](https://github.com/PennNGG/Quantitative-Neuroscience/tree/master/Answers%20to%20Exercises/Python) after the due date."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Response: We will compute the confidence / credible intervals using the four methods explored above: (1) Z-interval with known sigma,(2) t-interval (unknown sigma; uses sample s with Bessel correction, (3) Bootstrap percentile CI on the mean, and (4) Bayesian credible interval with flat prior & known sigma (== Z-interval). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configuration\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Method 1: Z-interval with known sigma\n",
    "def z_interval(xbar, n, sigma, alpha=0.05): \n",
    "    z = stats.norm.ppf(1 - alpha/2)\n",
    "    se = sigma / np.sqrt(n)\n",
    "    return xbar - z*se, xbar + z*se\n",
    "    \n",
    "# Method 2: t-interval\n",
    "def t_interval(x, alpha=0.05):\n",
    "    n = len(x)\n",
    "    xbar = np.mean(x)\n",
    "    s = np.std(x, ddof=1)\n",
    "    tcrit = stats.t.ppf(1 - alpha/2, df=n-1)\n",
    "    se = s / np.sqrt(n)\n",
    "    return xbar - tcrit*se, xbar + tcrit*se\n",
    "    \n",
    "# Method 3: Bootstrap percentile Cl\n",
    "def bootstrap_ci_mean(x, B=10000, alpha=0.05, rng=None):\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "    n = len(x)\n",
    "    idx = rng.integers(0, n, size=(B, n))\n",
    "    boot_means = x[idx].mean(axis=1)\n",
    "    lo, hi = np.percentile(boot_means, [100*alpha/2, 100*(1 - alpha/2)])\n",
    "    return lo, hi\n",
    "\n",
    "# Method 4: Bayesian Credible Interval\n",
    "def bayes_flat_known_sigma(xbar, n, sigma, alpha=0.05):\n",
    "    \"\"\"Flat prior on Î¼, known Ïƒ -> Normal posterior; identical to z-interval.\"\"\"\n",
    "    return z_interval(xbar, n, sigma, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sample Size   sample Mean  Z-interval: low  Z-interval: high  \\\n",
      "0             5       9.6019           7.8488           11.3549   \n",
      "1            10       9.7316           8.4920           10.9712   \n",
      "2            20       9.4737           8.5972           10.3503   \n",
      "3            40      10.6980          10.0783           11.3178   \n",
      "4            80       9.7929           9.3546           10.2311   \n",
      "5           160       9.9211           9.6112           10.2310   \n",
      "6          1000       9.9320           9.8081           10.0560   \n",
      "\n",
      "   T-interval: low  T-interval: high  Bootstrap: low  Bootstrap: high  \\\n",
      "0           6.5025           12.7012          7.5429          11.4747   \n",
      "1           8.2863           11.1769          8.6373          10.9698   \n",
      "2           8.3864           10.5610          8.5333          10.5075   \n",
      "3           9.9458           11.4503          9.9831          11.4042   \n",
      "4           9.2877           10.2980          9.2983          10.2922   \n",
      "5           9.5868           10.2554          9.5950          10.2510   \n",
      "6           9.8067           10.0573          9.8071          10.0572   \n",
      "\n",
      "   Bayesian Credible: low  Bayesian Credible: Bayes_high  \n",
      "0                  7.8488                        11.3549  \n",
      "1                  8.4920                        10.9712  \n",
      "2                  8.5972                        10.3503  \n",
      "3                 10.0783                        11.3178  \n",
      "4                  9.3546                        10.2311  \n",
      "5                  9.6112                        10.2310  \n",
      "6                  9.8081                        10.0560  \n"
     ]
    }
   ],
   "source": [
    "## Inputing values\n",
    "mu = 10.0 \n",
    "sigma = 2.0\n",
    "ns = [5, 10, 20, 40, 80, 160, 1000]\n",
    "alpha = 0.05\n",
    "B = 10000\n",
    "rng = np.random.default_rng(42)  # reproducible\n",
    "\n",
    "rows = []\n",
    "for n in ns:\n",
    "    x = rng.normal(mu, sigma, size=n)\n",
    "    xbar = float(np.mean(x))\n",
    "\n",
    "    z_lo, z_hi = z_interval(xbar, n, sigma, alpha)\n",
    "    t_lo, t_hi = t_interval(x, alpha)\n",
    "    b_lo, b_hi = bootstrap_ci_mean(x, B=B, alpha=alpha, rng=rng)\n",
    "    bayes_lo, bayes_hi = bayes_flat_known_sigma(xbar, n, sigma, alpha)\n",
    "\n",
    "    rows.append({\n",
    "        \"Sample Size \": n,\n",
    "        \"sample Mean\": xbar,\n",
    "        \"Z-interval: low\": z_lo, \"Z-interval: high\": z_hi,\n",
    "        \"T-interval: low\": t_lo, \"T-interval: high\": t_hi,\n",
    "        \"Bootstrap: low\": b_lo, \"Bootstrap: high\": b_hi,\n",
    "        \"Bayesian Credible: low\": bayes_lo, \"Bayesian Credible: Bayes_high\": bayes_hi\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(df.round(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tteEm2Qlgbb3"
   },
   "source": [
    "# Credits\n",
    "\n",
    "Copyright 2021 by Joshua I. Gold, University of Pennsylvania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "bkVu6eGKIIiQ"
   ],
   "name": "Confidence Intervals and Bootstrapping",
   "provenance": [
    {
     "file_id": "1HW0L_d5Wpod3jbnY3iG7mLhMG6yWHvF2",
     "timestamp": 1626350171621
    },
    {
     "file_id": "1-KxH3FCq5rDyyO33HXxewIv-kKldkINi",
     "timestamp": 1626290714843
    },
    {
     "file_id": "14S2ca44h8TKC1hFXjk5ktwBYpGU6R5S-",
     "timestamp": 1624411796822
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
